#!/usr/bin/env python3
"""
ç»“æœæ±‡æ€»è„šæœ¬ - åˆ†æprofilingå®éªŒçš„æ—¥å¿—æ–‡ä»¶å¹¶ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import os
import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime


def parse_log_file(log_path: str) -> Dict[str, Any]:
    """è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œæå–å…³é”®ä¿¡æ¯"""
    result = {
        "config": {},
        "status": "unknown",
        "model_params": 0,
        "memory_usage": {},
        "errors": [],
        "completion_time": None
    }
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–é…ç½®ä¿¡æ¯
        config_match = re.search(r'Trainer: (\w+), Batch Size: (\d+), Seq Length: (\d+), Model: (\w+)', content)
        if config_match:
            result["config"] = {
                "trainer": config_match.group(1),
                "batch_size": int(config_match.group(2)),
                "seq_length": int(config_match.group(3)),
                "model_size": config_match.group(4)
            }
        
        # æå–æ¨¡å‹å‚æ•°æ•°é‡
        params_match = re.search(r'Model parameters: ([\d,]+) \(([\d.]+)M\)', content)
        if params_match:
            result["model_params"] = int(params_match.group(1).replace(',', ''))
        
        # æå–å†…å­˜ä½¿ç”¨ä¿¡æ¯
        memory_patterns = {
            "initial": r'Initial Memory Usage:.*?GPU Allocated: ([\d.]+) MB.*?System RSS: ([\d.]+) MB',
            "after_model": r'After model creation Memory Usage:.*?GPU Allocated: ([\d.]+) MB.*?System RSS: ([\d.]+) MB',
            "after_profiling": r'After profiling Memory Usage:.*?GPU Allocated: ([\d.]+) MB.*?System RSS: ([\d.]+) MB',
            "after_cleanup": r'After cleanup Memory Usage:.*?GPU Allocated: ([\d.]+) MB.*?System RSS: ([\d.]+) MB'
        }
        
        for stage, pattern in memory_patterns.items():
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result["memory_usage"][stage] = {
                    "gpu_allocated_mb": float(match.group(1)),
                    "system_rss_mb": float(match.group(2))
                }
        
        # æ£€æŸ¥å®ŒæˆçŠ¶æ€
        if "Single configuration profiling completed successfully" in content:
            result["status"] = "success"
        elif "ERROR" in content or "Exception" in content or "Failed" in content:
            result["status"] = "failed"
            # æå–é”™è¯¯ä¿¡æ¯
            error_matches = re.findall(r'ERROR.*?(?=\n|$)', content)
            result["errors"] = error_matches[:5]  # æœ€å¤šä¿ç•™5ä¸ªé”™è¯¯
        
        # æå–å®Œæˆæ—¶é—´ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        time_match = re.search(r'Step \d+, Loss: ([\d.]+)', content)
        if time_match:
            result["completion_time"] = datetime.now().isoformat()
    
    except Exception as e:
        result["status"] = "parse_error"
        result["errors"] = [f"Failed to parse log file: {str(e)}"]
    
    return result


def generate_summary_report(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    summary = {
        "total_experiments": len(results),
        "successful": 0,
        "failed": 0,
        "parse_errors": 0,
        "by_trainer": {},
        "memory_analysis": {},
        "failed_experiments": []
    }
    
    for result in results:
        # ç»Ÿè®¡çŠ¶æ€
        if result["status"] == "success":
            summary["successful"] += 1
        elif result["status"] == "failed":
            summary["failed"] += 1
            summary["failed_experiments"].append({
                "config": result["config"],
                "errors": result["errors"]
            })
        else:
            summary["parse_errors"] += 1
        
        # æŒ‰traineråˆ†ç±»ç»Ÿè®¡
        trainer = result["config"].get("trainer", "unknown")
        if trainer not in summary["by_trainer"]:
            summary["by_trainer"][trainer] = {
                "total": 0,
                "successful": 0,
                "failed": 0,
                "avg_model_params": 0,
                "memory_usage": []
            }
        
        summary["by_trainer"][trainer]["total"] += 1
        if result["status"] == "success":
            summary["by_trainer"][trainer]["successful"] += 1
        elif result["status"] == "failed":
            summary["by_trainer"][trainer]["failed"] += 1
        
        # æ”¶é›†å†…å­˜ä½¿ç”¨æ•°æ®
        if result["memory_usage"]:
            summary["by_trainer"][trainer]["memory_usage"].append(result["memory_usage"])
    
    # è®¡ç®—å¹³å‡å†…å­˜ä½¿ç”¨
    for trainer_data in summary["by_trainer"].values():
        if trainer_data["memory_usage"]:
            # è®¡ç®—å¹³å‡GPUå†…å­˜ä½¿ç”¨
            gpu_usage = []
            for mem_data in trainer_data["memory_usage"]:
                if "after_profiling" in mem_data:
                    gpu_usage.append(mem_data["after_profiling"]["gpu_allocated_mb"])
            
            if gpu_usage:
                trainer_data["avg_gpu_memory_mb"] = sum(gpu_usage) / len(gpu_usage)
    
    return summary


def print_summary_report(summary: Dict[str, Any]):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    print("=" * 60)
    print("ğŸ¯ PROFILING EXPERIMENTS SUMMARY REPORT")
    print("=" * 60)
    print(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æ€»ä½“ç»Ÿè®¡
    print("ğŸ“Š Overall Statistics:")
    print(f"  Total experiments: {summary['total_experiments']}")
    print(f"  Successful: {summary['successful']} ({summary['successful']/summary['total_experiments']*100:.1f}%)")
    print(f"  Failed: {summary['failed']} ({summary['failed']/summary['total_experiments']*100:.1f}%)")
    if summary['parse_errors'] > 0:
        print(f"  Parse errors: {summary['parse_errors']}")
    print()
    
    # æŒ‰trainerç»Ÿè®¡
    print("ğŸ” Results by Trainer:")
    for trainer, data in summary["by_trainer"].items():
        success_rate = data["successful"] / data["total"] * 100 if data["total"] > 0 else 0
        print(f"  {trainer}:")
        print(f"    Success rate: {data['successful']}/{data['total']} ({success_rate:.1f}%)")
        if "avg_gpu_memory_mb" in data:
            print(f"    Avg GPU memory: {data['avg_gpu_memory_mb']:.1f} MB")
    print()
    
    # å¤±è´¥çš„å®éªŒ
    if summary["failed_experiments"]:
        print("âŒ Failed Experiments:")
        for i, failed in enumerate(summary["failed_experiments"][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            config = failed["config"]
            print(f"  {i}. {config.get('trainer', 'unknown')} "
                  f"(bs={config.get('batch_size', '?')}, seq={config.get('seq_length', '?')})")
            if failed["errors"]:
                print(f"     Error: {failed['errors'][0][:100]}...")
        
        if len(summary["failed_experiments"]) > 5:
            print(f"     ... and {len(summary['failed_experiments']) - 5} more")
        print()
    
    print("ğŸ“‹ For detailed results, check individual log files in ./logs/")
    print("ğŸ“Š View TensorBoard results: tensorboard --logdir=./runs")


def main():
    parser = argparse.ArgumentParser(description="Summarize profiling experiment results")
    parser.add_argument("--logs-dir", type=str, default="logs",
                       help="Directory containing log files")
    parser.add_argument("--output", type=str, 
                       help="Output JSON file for detailed results")
    
    args = parser.parse_args()
    
    logs_dir = Path(args.logs_dir)
    if not logs_dir.exists():
        print(f"Error: Logs directory '{logs_dir}' not found")
        return 1
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    log_files = list(logs_dir.glob("*.log"))
    if not log_files:
        print(f"No log files found in '{logs_dir}'")
        return 1
    
    print(f"Found {len(log_files)} log files to analyze...")
    
    # è§£ææ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    results = []
    for log_file in log_files:
        print(f"Parsing {log_file.name}...")
        result = parse_log_file(str(log_file))
        result["log_file"] = str(log_file)
        results.append(result)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = generate_summary_report(results)
    
    # æ‰“å°æŠ¥å‘Š
    print_summary_report(summary)
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
    if args.output:
        output_data = {
            "summary": summary,
            "detailed_results": results,
            "generated_at": datetime.now().isoformat()
        }
        
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Detailed results saved to: {args.output}")
    
    return 0


if __name__ == "__main__":
    exit(main())