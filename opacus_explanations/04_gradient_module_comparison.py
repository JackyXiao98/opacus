#!/usr/bin/env python3
"""
GradSampleModule vs GradSampleModuleFastGradientClipping è¯¦ç»†å¯¹æ¯”

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†ä¸¤ç§ä¸åŒçš„per-sample gradientè®¡ç®—æ–¹æ³•çš„åŒºåˆ«ï¼š
1. æ ‡å‡†ç‰ˆæœ¬ï¼šè®¡ç®—å®Œæ•´çš„per-sample gradients
2. å¿«é€Ÿç‰ˆæœ¬ï¼šåªè®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼Œæ”¯æŒGhost Clippingå’ŒFast Gradient Clipping
"""

import torch
import torch.nn as nn
import tracemalloc
from opacus.grad_sample import GradSampleModule

def create_test_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    return nn.Sequential(
        nn.Linear(100, 50),
        nn.ReLU(),
        nn.Linear(50, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )

def create_test_data(batch_size=32, input_dim=100):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    X = torch.randn(batch_size, input_dim)
    y = torch.randn(batch_size, 1)
    return X, y

def compare_basic_functionality():
    """å¯¹æ¯”åŸºæœ¬åŠŸèƒ½å·®å¼‚"""
    print("=" * 80)
    print("1. åŸºæœ¬åŠŸèƒ½å¯¹æ¯”")
    print("=" * 80)
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„æ¨¡å‹
    model1 = create_test_model()
    model2 = create_test_model()
    
    # ç¡®ä¿å‚æ•°ç›¸åŒ
    with torch.no_grad():
        for p1, p2 in zip(model1.parameters(), model2.parameters()):
            p2.copy_(p1)
    
    # åŒ…è£…æˆä¸åŒçš„GradSampleModule
    grad_sample_module = GradSampleModule(model1)
    
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥å¯¼å…¥GradSampleModuleFastGradientClipping
    # å› ä¸ºå®ƒå¯èƒ½åœ¨ä¸åŒçš„æ–‡ä»¶ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ¦‚å¿µæ€§çš„å¯¹æ¯”
    
    print("æ ‡å‡†GradSampleModuleç‰¹ç‚¹:")
    print("- è®¡ç®—å®Œæ•´çš„per-sample gradients")
    print("- å­˜å‚¨åœ¨å‚æ•°çš„grad_sampleå±æ€§ä¸­")
    print("- å½¢çŠ¶: [batch_size, *param_shape]")
    print("- å†…å­˜ä½¿ç”¨: é«˜ï¼ˆå­˜å‚¨æ‰€æœ‰æ¢¯åº¦ï¼‰")
    
    print("\nFastGradientClippingç‰ˆæœ¬ç‰¹ç‚¹:")
    print("- åªè®¡ç®—æ¢¯åº¦èŒƒæ•°")
    print("- å­˜å‚¨åœ¨å‚æ•°çš„_norm_sampleå±æ€§ä¸­")
    print("- å½¢çŠ¶: [batch_size] (åªæœ‰èŒƒæ•°)")
    print("- å†…å­˜ä½¿ç”¨: ä½ï¼ˆåªå­˜å‚¨èŒƒæ•°ï¼‰")
    
    # æµ‹è¯•æ ‡å‡†ç‰ˆæœ¬
    X, y = create_test_data(batch_size=4)
    output = grad_sample_module(X)
    loss = nn.MSELoss()(output, y)
    loss.backward()
    
    print(f"\næ ‡å‡†ç‰ˆæœ¬ç»“æœ:")
    for name, param in grad_sample_module.named_parameters():
        if hasattr(param, 'grad_sample'):
            print(f"  {name}: grad_sampleå½¢çŠ¶ = {param.grad_sample.shape}")
            # è®¡ç®—èŒƒæ•°ç”¨äºå¯¹æ¯”
            norms = param.grad_sample.reshape(param.grad_sample.shape[0], -1).norm(2, dim=1)
            print(f"    è®¡ç®—å¾—åˆ°çš„èŒƒæ•°: {norms}")

def explain_memory_efficiency():
    """è§£é‡Šå†…å­˜æ•ˆç‡å·®å¼‚"""
    print(f"\n" + "=" * 80)
    print("2. å†…å­˜æ•ˆç‡å¯¹æ¯”")
    print("=" * 80)
    
    batch_size = 32
    param_sizes = {
        "layer1.weight": (50, 100),    # 5000 parameters
        "layer1.bias": (50,),          # 50 parameters  
        "layer2.weight": (10, 50),     # 500 parameters
        "layer2.bias": (10,),          # 10 parameters
        "layer3.weight": (1, 10),      # 10 parameters
        "layer3.bias": (1,),           # 1 parameter
    }
    
    print("å†…å­˜ä½¿ç”¨ä¼°ç®— (å‡è®¾float32, 4 bytes per element):")
    print(f"Batch size: {batch_size}")
    
    total_standard = 0
    total_fast = 0
    
    for name, shape in param_sizes.items():
        param_count = torch.tensor(shape).prod().item()
        
        # æ ‡å‡†ç‰ˆæœ¬ï¼šå­˜å‚¨å®Œæ•´æ¢¯åº¦ [batch_size, *param_shape]
        standard_memory = batch_size * param_count * 4  # bytes
        
        # å¿«é€Ÿç‰ˆæœ¬ï¼šåªå­˜å‚¨èŒƒæ•° [batch_size]
        fast_memory = batch_size * 4  # bytes
        
        total_standard += standard_memory
        total_fast += fast_memory
        
        print(f"\n{name}:")
        print(f"  å‚æ•°å½¢çŠ¶: {shape} ({param_count} å‚æ•°)")
        print(f"  æ ‡å‡†ç‰ˆæœ¬: {batch_size} Ã— {param_count} Ã— 4 = {standard_memory:,} bytes")
        print(f"  å¿«é€Ÿç‰ˆæœ¬: {batch_size} Ã— 1 Ã— 4 = {fast_memory:,} bytes")
        print(f"  èŠ‚çœ: {(1 - fast_memory/standard_memory)*100:.1f}%")
    
    print(f"\næ€»è®¡:")
    print(f"  æ ‡å‡†ç‰ˆæœ¬æ€»å†…å­˜: {total_standard:,} bytes ({total_standard/1024/1024:.2f} MB)")
    print(f"  å¿«é€Ÿç‰ˆæœ¬æ€»å†…å­˜: {total_fast:,} bytes ({total_fast/1024:.2f} KB)")
    print(f"  æ€»ä½“èŠ‚çœ: {(1 - total_fast/total_standard)*100:.1f}%")

def explain_ghost_clipping():
    """è§£é‡ŠGhost Clippingæœºåˆ¶"""
    print(f"\n" + "=" * 80)
    print("3. Ghost Clipping vs Fast Gradient Clipping")
    print("=" * 80)
    
    explanation = """
    FastGradientClippingç‰ˆæœ¬æ”¯æŒä¸¤ç§ä¼˜åŒ–æ¨¡å¼:
    
    ğŸ”¥ Ghost Clipping (use_ghost_clipping=True):
    ----------------------------------------
    â€¢ åŸç†: ç›´æ¥ä»æ¿€æ´»å€¼å’Œåå‘æ¢¯åº¦è®¡ç®—èŒƒæ•°ï¼Œæ— éœ€è®¡ç®—å®Œæ•´æ¢¯åº¦
    â€¢ æ”¯æŒå±‚: æœ‰ä¸“é—¨NORM_SAMPLERSçš„å±‚ï¼ˆå¦‚Linear, Conv2dç­‰ï¼‰
    â€¢ å†…å­˜æ•ˆç‡: æœ€é«˜ï¼ˆå®Œå…¨é¿å…æ¢¯åº¦å®ä¾‹åŒ–ï¼‰
    â€¢ è®¡ç®—æ•ˆç‡: æœ€é«˜ï¼ˆä¸“é—¨ä¼˜åŒ–çš„èŒƒæ•°è®¡ç®—ï¼‰
    â€¢ é™åˆ¶: ä¸æ”¯æŒå‚æ•°å…±äº«(parameter tying)
    
    âš¡ Fast Gradient Clipping (use_ghost_clipping=False):
    --------------------------------------------------
    â€¢ åŸç†: å…ˆè®¡ç®—å®Œæ•´æ¢¯åº¦ï¼Œç„¶åç«‹å³è®¡ç®—èŒƒæ•°å¹¶ä¸¢å¼ƒæ¢¯åº¦
    â€¢ æ”¯æŒå±‚: æ‰€æœ‰å±‚ï¼ˆä½¿ç”¨GRAD_SAMPLERSæˆ–functorchï¼‰
    â€¢ å†…å­˜æ•ˆç‡: ä¸­ç­‰ï¼ˆä¸´æ—¶å­˜å‚¨æ¢¯åº¦ï¼‰
    â€¢ è®¡ç®—æ•ˆç‡: ä¸­ç­‰ï¼ˆéœ€è¦å®Œæ•´æ¢¯åº¦è®¡ç®—ï¼‰
    â€¢ é™åˆ¶: è¾ƒå°‘
    
    ğŸ“Š æ ‡å‡†æ–¹æ³• (GradSampleModule):
    -----------------------------
    â€¢ åŸç†: è®¡ç®—å¹¶æŒä¹…å­˜å‚¨å®Œæ•´çš„per-sampleæ¢¯åº¦
    â€¢ æ”¯æŒå±‚: æ‰€æœ‰å±‚
    â€¢ å†…å­˜æ•ˆç‡: æœ€ä½ï¼ˆå­˜å‚¨æ‰€æœ‰æ¢¯åº¦ï¼‰
    â€¢ è®¡ç®—æ•ˆç‡: æœ€ä½ï¼ˆéœ€è¦å­˜å‚¨å¤§é‡æ•°æ®ï¼‰
    â€¢ é™åˆ¶: æ— ï¼ˆæœ€é€šç”¨ï¼‰
    """
    print(explanation)

def demonstrate_workflow_differences():
    """æ¼”ç¤ºå·¥ä½œæµç¨‹å·®å¼‚"""
    print(f"\n" + "=" * 80)
    print("4. å·¥ä½œæµç¨‹å·®å¼‚")
    print("=" * 80)
    
    print("æ ‡å‡†GradSampleModuleå·¥ä½œæµç¨‹:")
    print("1. å‰å‘ä¼ æ’­ â†’ ä¿å­˜æ¿€æ´»å€¼")
    print("2. åå‘ä¼ æ’­ â†’ è®¡ç®—per-sampleæ¢¯åº¦")
    print("3. å­˜å‚¨æ¢¯åº¦ â†’ param.grad_sample = [batch_size, *param_shape]")
    print("4. ä¼˜åŒ–å™¨ä½¿ç”¨ â†’ ä»grad_sampleè®¡ç®—èŒƒæ•°ã€è£å‰ªã€èšåˆ")
    
    print(f"\nFastGradientClippingå·¥ä½œæµç¨‹:")
    print("Ghost Clippingæ¨¡å¼:")
    print("1. å‰å‘ä¼ æ’­ â†’ ä¿å­˜æ¿€æ´»å€¼")
    print("2. åå‘ä¼ æ’­ â†’ ç›´æ¥è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆæ— æ¢¯åº¦å®ä¾‹åŒ–ï¼‰")
    print("3. å­˜å‚¨èŒƒæ•° â†’ param._norm_sample = [batch_size]")
    print("4. ä¼˜åŒ–å™¨ä½¿ç”¨ â†’ ç›´æ¥ä½¿ç”¨èŒƒæ•°è¿›è¡Œè£å‰ª")
    
    print(f"\nFast Gradient Clippingæ¨¡å¼:")
    print("1. å‰å‘ä¼ æ’­ â†’ ä¿å­˜æ¿€æ´»å€¼")
    print("2. åå‘ä¼ æ’­ â†’ è®¡ç®—per-sampleæ¢¯åº¦")
    print("3. è®¡ç®—èŒƒæ•° â†’ ç«‹å³ä»æ¢¯åº¦è®¡ç®—èŒƒæ•°")
    print("4. ä¸¢å¼ƒæ¢¯åº¦ â†’ param.grad_sample = None")
    print("5. å­˜å‚¨èŒƒæ•° â†’ param._norm_sample = [batch_size]")
    print("6. ä¼˜åŒ–å™¨ä½¿ç”¨ â†’ ä½¿ç”¨èŒƒæ•°è¿›è¡Œè£å‰ª")

def compare_use_cases():
    """å¯¹æ¯”ä½¿ç”¨åœºæ™¯"""
    print(f"\n" + "=" * 80)
    print("5. é€‚ç”¨åœºæ™¯å¯¹æ¯”")
    print("=" * 80)
    
    scenarios = {
        "æ ‡å‡†GradSampleModule": {
            "é€‚ç”¨åœºæ™¯": [
                "éœ€è¦å®Œæ•´per-sampleæ¢¯åº¦ä¿¡æ¯",
                "ç ”ç©¶å’Œè°ƒè¯•ç›®çš„",
                "è‡ªå®šä¹‰æ¢¯åº¦å¤„ç†é€»è¾‘",
                "å°æ¨¡å‹æˆ–å†…å­˜å……è¶³çš„æƒ…å†µ"
            ],
            "ä¼˜åŠ¿": [
                "åŠŸèƒ½æœ€å®Œæ•´",
                "æœ€å¤§çµæ´»æ€§",
                "æ”¯æŒæ‰€æœ‰æ“ä½œ"
            ],
            "åŠ£åŠ¿": [
                "å†…å­˜ä½¿ç”¨é‡å¤§",
                "è®¡ç®—å¼€é”€é«˜",
                "å¯èƒ½å¯¼è‡´OOM"
            ]
        },
        "FastGradientClipping": {
            "é€‚ç”¨åœºæ™¯": [
                "åªéœ€è¦æ¢¯åº¦è£å‰ªåŠŸèƒ½",
                "å¤§æ¨¡å‹è®­ç»ƒ",
                "å†…å­˜å—é™ç¯å¢ƒ",
                "ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²"
            ],
            "ä¼˜åŠ¿": [
                "å†…å­˜æ•ˆç‡æé«˜",
                "è®¡ç®—é€Ÿåº¦å¿«",
                "æ”¯æŒGhost Clippingä¼˜åŒ–"
            ],
            "åŠ£åŠ¿": [
                "åŠŸèƒ½ç›¸å¯¹å—é™",
                "ä¸æ”¯æŒå‚æ•°å…±äº«(Ghostæ¨¡å¼)",
                "è°ƒè¯•ä¿¡æ¯è¾ƒå°‘"
            ]
        }
    }
    
    for method, info in scenarios.items():
        print(f"\n{method}:")
        print(f"  é€‚ç”¨åœºæ™¯:")
        for scenario in info["é€‚ç”¨åœºæ™¯"]:
            print(f"    â€¢ {scenario}")
        print(f"  ä¼˜åŠ¿:")
        for advantage in info["ä¼˜åŠ¿"]:
            print(f"    âœ… {advantage}")
        print(f"  åŠ£åŠ¿:")
        for disadvantage in info["åŠ£åŠ¿"]:
            print(f"    âŒ {disadvantage}")

def explain_implementation_differences():
    """è§£é‡Šå®ç°å·®å¼‚"""
    print(f"\n" + "=" * 80)
    print("6. å…³é”®å®ç°å·®å¼‚")
    print("=" * 80)
    
    print("capture_backprops_hookæ–¹æ³•å·®å¼‚:")
    print(f"\næ ‡å‡†ç‰ˆæœ¬:")
    print("```python")
    print("# æ€»æ˜¯è®¡ç®—å®Œæ•´æ¢¯åº¦")
    print("grad_samples = grad_sampler_fn(module, activations, backprops)")
    print("for param, gs in grad_samples.items():")
    print("    create_or_accumulate_grad_sample(param=param, grad_sample=gs)")
    print("```")
    
    print(f"\nå¿«é€Ÿç‰ˆæœ¬:")
    print("```python")
    print("if self.use_ghost_clipping and type(module) in self.NORM_SAMPLERS:")
    print("    # Ghost Clipping: ç›´æ¥è®¡ç®—èŒƒæ•°")
    print("    norm_sampler_fn = self.NORM_SAMPLERS[type(module)]")
    print("    norm_samples = norm_sampler_fn(module, activations, backprops)")
    print("    for param, ns in norm_samples.items():")
    print("        param._norm_sample = ns")
    print("else:")
    print("    # Fast Gradient Clipping: è®¡ç®—æ¢¯åº¦åç«‹å³è½¬æ¢ä¸ºèŒƒæ•°")
    print("    grad_samples = grad_sampler_fn(module, activations, backprops)")
    print("    # ... è®¡ç®—èŒƒæ•°å¹¶ä¸¢å¼ƒæ¢¯åº¦")
    print("    create_norm_sample(param=p, grad_sample=p.grad_sample)")
    print("    p.grad_sample = None  # ç«‹å³é‡Šæ”¾å†…å­˜")
    print("```")
    
    print(f"\næ–°å¢æ–¹æ³•:")
    print("â€¢ get_norm_sample(): è·å–per-sampleæ¢¯åº¦èŒƒæ•°")
    print("â€¢ get_clipping_coef(): è®¡ç®—è£å‰ªç³»æ•°")
    print("â€¢ NORM_SAMPLERS: ä¸“é—¨çš„èŒƒæ•°è®¡ç®—å™¨æ³¨å†Œè¡¨")

if __name__ == "__main__":
    print("GradSampleModule vs GradSampleModuleFastGradientClipping è¯¦ç»†å¯¹æ¯”")
    print("=" * 80)
    
    compare_basic_functionality()
    explain_memory_efficiency()
    explain_ghost_clipping()
    demonstrate_workflow_differences()
    compare_use_cases()
    explain_implementation_differences()
    
    print(f"\n" + "=" * 80)
    print("æ€»ç»“")
    print("=" * 80)
    print("é€‰æ‹©å»ºè®®:")
    print("â€¢ å¦‚æœåªéœ€è¦æ¢¯åº¦è£å‰ª â†’ ä½¿ç”¨ FastGradientClipping")
    print("â€¢ å¦‚æœéœ€è¦å®Œæ•´æ¢¯åº¦ä¿¡æ¯ â†’ ä½¿ç”¨æ ‡å‡† GradSampleModule") 
    print("â€¢ å¦‚æœå†…å­˜å—é™ â†’ ä¼˜å…ˆè€ƒè™‘ FastGradientClipping + Ghost Clipping")
    print("â€¢ å¦‚æœæœ‰å‚æ•°å…±äº« â†’ ä½¿ç”¨ FastGradientClipping (éGhostæ¨¡å¼)")
    print("=" * 80)