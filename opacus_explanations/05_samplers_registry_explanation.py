#!/usr/bin/env python3
"""
GRAD_SAMPLERS å’Œ NORM_SAMPLERS æ³¨å†Œæœºåˆ¶è¯¦è§£

è¿™ä¸ªç¤ºä¾‹è¯¦ç»†è§£é‡Šäº†Opacusä¸­ä¸¤ä¸ªæ ¸å¿ƒæ³¨å†Œè¡¨çš„å·¥ä½œåŸç†ï¼š
1. GRAD_SAMPLERS: ç”¨äºæ³¨å†Œå®Œæ•´æ¢¯åº¦è®¡ç®—å‡½æ•°
2. NORM_SAMPLERS: ç”¨äºæ³¨å†Œæ¢¯åº¦èŒƒæ•°è®¡ç®—å‡½æ•°ï¼ˆGhost Clippingï¼‰

é‡ç‚¹è§£é‡Šç¬¬217è¡Œå’Œç¬¬227è¡Œä»£ç çš„ä½œç”¨æœºåˆ¶ã€‚
"""

import torch
import torch.nn as nn
from opacus.grad_sample import GradSampleModule
from opacus.grad_sample.grad_sample_module_fast_gradient_clipping import GradSampleModuleFastGradientClipping

def explain_samplers_registry():
    """è§£é‡Šé‡‡æ ·å™¨æ³¨å†Œè¡¨çš„åŸºæœ¬æ¦‚å¿µ"""
    print("=" * 80)
    print("GRAD_SAMPLERS å’Œ NORM_SAMPLERS æ³¨å†Œè¡¨è§£é‡Š")
    print("=" * 80)
    
    explanation = """
    Opacusä½¿ç”¨ä¸¤ä¸ªæ³¨å†Œè¡¨æ¥ç®¡ç†ä¸åŒå±‚ç±»å‹çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼š
    
    ğŸ“‹ GRAD_SAMPLERS (æ¢¯åº¦é‡‡æ ·å™¨æ³¨å†Œè¡¨):
    --------------------------------
    â€¢ ä½œç”¨: å­˜å‚¨æ¯ç§å±‚ç±»å‹å¯¹åº”çš„å®Œæ•´per-sampleæ¢¯åº¦è®¡ç®—å‡½æ•°
    â€¢ ä½ç½®: GradSampleModule.GRAD_SAMPLERS
    â€¢ ç”¨é€”: è®¡ç®—å®Œæ•´çš„per-sampleæ¢¯åº¦ [batch_size, *param_shape]
    â€¢ ç¤ºä¾‹: nn.Linear â†’ compute_linear_grad_sample()
    
    ğŸš€ NORM_SAMPLERS (èŒƒæ•°é‡‡æ ·å™¨æ³¨å†Œè¡¨):
    ----------------------------------
    â€¢ ä½œç”¨: å­˜å‚¨æ¯ç§å±‚ç±»å‹å¯¹åº”çš„æ¢¯åº¦èŒƒæ•°è®¡ç®—å‡½æ•°
    â€¢ ä½ç½®: GradSampleModuleFastGradientClipping.NORM_SAMPLERS
    â€¢ ç”¨é€”: ç›´æ¥è®¡ç®—æ¢¯åº¦èŒƒæ•° [batch_size] (Ghost Clipping)
    â€¢ ç¤ºä¾‹: nn.Linear â†’ compute_linear_norm_sample()
    
    ğŸ”§ æ³¨å†Œæœºåˆ¶:
    -----------
    â€¢ ä½¿ç”¨è£…é¥°å™¨ @register_grad_sampler å’Œ @register_norm_sampler
    â€¢ åœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨æ³¨å†Œåˆ°å¯¹åº”çš„å­—å…¸ä¸­
    â€¢ æ”¯æŒä¸€ä¸ªå‡½æ•°æ³¨å†Œåˆ°å¤šä¸ªå±‚ç±»å‹
    """
    print(explanation)

def demonstrate_registry_content():
    """æ¼”ç¤ºæ³¨å†Œè¡¨çš„å†…å®¹"""
    print(f"\n" + "=" * 80)
    print("æ³¨å†Œè¡¨å†…å®¹æ¼”ç¤º")
    print("=" * 80)
    
    # æŸ¥çœ‹GRAD_SAMPLERSçš„å†…å®¹
    print("GRAD_SAMPLERS æ³¨å†Œçš„å±‚ç±»å‹:")
    for layer_type, sampler_func in GradSampleModule.GRAD_SAMPLERS.items():
        print(f"  {layer_type.__name__}: {sampler_func.__name__}")
    
    print(f"\nNORM_SAMPLERS æ³¨å†Œçš„å±‚ç±»å‹:")
    for layer_type, sampler_func in GradSampleModuleFastGradientClipping.NORM_SAMPLERS.items():
        print(f"  {layer_type.__name__}: {sampler_func.__name__}")

def explain_line_217_and_227():
    """è¯¦ç»†è§£é‡Šç¬¬217è¡Œå’Œç¬¬227è¡Œä»£ç çš„ä½œç”¨"""
    print(f"\n" + "=" * 80)
    print("ç¬¬217è¡Œå’Œç¬¬227è¡Œä»£ç è¯¦è§£")
    print("=" * 80)
    
    code_explanation = """
    è¿™ä¸¤è¡Œä»£ç ä½äº capture_backprops_hook æ–¹æ³•ä¸­ï¼Œè´Ÿè´£é€‰æ‹©åˆé€‚çš„é‡‡æ ·å™¨å‡½æ•°ï¼š
    
    ğŸ¯ ç¬¬217è¡Œ: norm_sampler_fn = self.NORM_SAMPLERS[type(module)]
    --------------------------------------------------------
    â€¢ ä½ç½®: Ghost Clippingåˆ†æ”¯ä¸­
    â€¢ ä½œç”¨: ä»NORM_SAMPLERSæ³¨å†Œè¡¨ä¸­è·å–å½“å‰æ¨¡å—ç±»å‹å¯¹åº”çš„èŒƒæ•°è®¡ç®—å‡½æ•°
    â€¢ æ¡ä»¶: self.use_ghost_clipping=True ä¸” type(module) in self.NORM_SAMPLERS
    â€¢ ç»“æœ: ç›´æ¥è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼Œæ— éœ€å®Œæ•´æ¢¯åº¦å®ä¾‹åŒ–
    
    ğŸ¯ ç¬¬227è¡Œ: grad_sampler_fn = self.GRAD_SAMPLERS[type(module)]
    ---------------------------------------------------------
    â€¢ ä½ç½®: Fast Gradient Clippingåˆ†æ”¯ä¸­
    â€¢ ä½œç”¨: ä»GRAD_SAMPLERSæ³¨å†Œè¡¨ä¸­è·å–å½“å‰æ¨¡å—ç±»å‹å¯¹åº”çš„æ¢¯åº¦è®¡ç®—å‡½æ•°
    â€¢ æ¡ä»¶: ä¸ä½¿ç”¨Ghost Clipping ä¸” type(module) in self.GRAD_SAMPLERS
    â€¢ ç»“æœ: è®¡ç®—å®Œæ•´per-sampleæ¢¯åº¦ï¼Œç„¶åè½¬æ¢ä¸ºèŒƒæ•°
    
    ğŸ”„ å†³ç­–æµç¨‹:
    -----------
    if self.use_ghost_clipping and type(module) in self.NORM_SAMPLERS:
        # ç¬¬217è¡Œ: ä½¿ç”¨Ghost Clipping (æœ€é«˜æ•ˆ)
        norm_sampler_fn = self.NORM_SAMPLERS[type(module)]
        norm_samples = norm_sampler_fn(module, activations, backprops)
    else:
        if not self.force_functorch and type(module) in self.GRAD_SAMPLERS:
            # ç¬¬227è¡Œ: ä½¿ç”¨ä¸“é—¨çš„æ¢¯åº¦é‡‡æ ·å™¨
            grad_sampler_fn = self.GRAD_SAMPLERS[type(module)]
        else:
            # ä½¿ç”¨functorché€šç”¨æ–¹æ³•
            grad_sampler_fn = ft_compute_per_sample_gradient
    """
    print(code_explanation)

def demonstrate_sampler_selection():
    """æ¼”ç¤ºé‡‡æ ·å™¨é€‰æ‹©è¿‡ç¨‹"""
    print(f"\n" + "=" * 80)
    print("é‡‡æ ·å™¨é€‰æ‹©è¿‡ç¨‹æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºä¸åŒç±»å‹çš„æ¨¡å—
    modules = {
        "Linear": nn.Linear(10, 5),
        "Conv2d": nn.Conv2d(3, 16, 3),
        "ReLU": nn.ReLU(),
        "BatchNorm2d": nn.BatchNorm2d(16)
    }
    
    print("ä¸åŒæ¨¡å—ç±»å‹çš„é‡‡æ ·å™¨æ”¯æŒæƒ…å†µ:")
    print(f"{'æ¨¡å—ç±»å‹':<15} {'GRAD_SAMPLERS':<15} {'NORM_SAMPLERS':<15} {'é€‰æ‹©ç­–ç•¥'}")
    print("-" * 70)
    
    for name, module in modules.items():
        module_type = type(module)
        has_grad_sampler = module_type in GradSampleModule.GRAD_SAMPLERS
        has_norm_sampler = module_type in GradSampleModuleFastGradientClipping.NORM_SAMPLERS
        
        if has_norm_sampler:
            strategy = "Ghost Clipping (æœ€ä¼˜)"
        elif has_grad_sampler:
            strategy = "Fast Gradient Clipping"
        else:
            strategy = "Functorch (é€šç”¨)"
        
        print(f"{name:<15} {'âœ…' if has_grad_sampler else 'âŒ':<15} {'âœ…' if has_norm_sampler else 'âŒ':<15} {strategy}")

def show_actual_sampler_functions():
    """å±•ç¤ºå®é™…çš„é‡‡æ ·å™¨å‡½æ•°"""
    print(f"\n" + "=" * 80)
    print("å®é™…é‡‡æ ·å™¨å‡½æ•°ç¤ºä¾‹")
    print("=" * 80)
    
    # è·å–Linearå±‚çš„é‡‡æ ·å™¨å‡½æ•°
    linear_grad_sampler = GradSampleModule.GRAD_SAMPLERS.get(nn.Linear)
    linear_norm_sampler = GradSampleModuleFastGradientClipping.NORM_SAMPLERS.get(nn.Linear)
    
    print("Linearå±‚çš„é‡‡æ ·å™¨å‡½æ•°:")
    if linear_grad_sampler:
        print(f"  GRAD_SAMPLER: {linear_grad_sampler.__name__}")
        print(f"    æ–‡ä»¶ä½ç½®: {linear_grad_sampler.__module__}")
        print(f"    å‡½æ•°ç­¾å: {linear_grad_sampler.__name__}(layer, activations, backprops)")
        print(f"    è¿”å›ç±»å‹: Dict[nn.Parameter, torch.Tensor] (å®Œæ•´æ¢¯åº¦)")
    
    if linear_norm_sampler:
        print(f"\n  NORM_SAMPLER: {linear_norm_sampler.__name__}")
        print(f"    æ–‡ä»¶ä½ç½®: {linear_norm_sampler.__module__}")
        print(f"    å‡½æ•°ç­¾å: {linear_norm_sampler.__name__}(layer, activations, backprops)")
        print(f"    è¿”å›ç±»å‹: Dict[nn.Parameter, torch.Tensor] (æ¢¯åº¦èŒƒæ•°)")

def explain_registration_process():
    """è§£é‡Šæ³¨å†Œè¿‡ç¨‹"""
    print(f"\n" + "=" * 80)
    print("é‡‡æ ·å™¨æ³¨å†Œè¿‡ç¨‹")
    print("=" * 80)
    
    registration_code = '''
    # åœ¨ opacus/grad_sample/linear.py ä¸­:
    
    from .utils import register_grad_sampler, register_norm_sampler
    
    @register_grad_sampler(nn.Linear)
    def compute_linear_grad_sample(layer, activations, backprops):
        """è®¡ç®—Linearå±‚çš„å®Œæ•´per-sampleæ¢¯åº¦"""
        activations = activations[0]
        ret = {}
        if layer.weight.requires_grad:
            # ä½¿ç”¨Einsteinæ±‚å’Œè®¡ç®—æ¢¯åº¦: backprops âŠ— activations
            gs = torch.einsum("n...i,n...j->nij", backprops, activations)
            ret[layer.weight] = gs
        if layer.bias is not None and layer.bias.requires_grad:
            ret[layer.bias] = torch.einsum("n...k->nk", backprops)
        return ret
    
    @register_norm_sampler(nn.Linear)
    def compute_linear_norm_sample(layer, activations, backprops):
        """è®¡ç®—Linearå±‚çš„æ¢¯åº¦èŒƒæ•° (Ghost Clipping)"""
        activations = activations[0]
        ret = {}
        if layer.weight.requires_grad:
            # ç›´æ¥è®¡ç®—èŒƒæ•°: ||grad|| = sqrt(||backprops||Â² * ||activations||Â²)
            g = torch.einsum("n...i,n...i->n", backprops, backprops)
            a = torch.einsum("n...j,n...j->n", activations, activations)
            ret[layer.weight] = torch.sqrt((g * a).flatten())
        if layer.bias is not None and layer.bias.requires_grad:
            ret[layer.bias] = torch.sqrt(
                torch.einsum("n...i,n...i->n", backprops, backprops).flatten()
            )
        return ret
    '''
    
    print("æ³¨å†Œè£…é¥°å™¨çš„å·¥ä½œåŸç†:")
    print(registration_code)
    
    print(f"\næ³¨å†Œè£…é¥°å™¨å®ç° (åœ¨ utils.py ä¸­):")
    decorator_code = '''
    def register_grad_sampler(target_class_or_classes):
        def decorator(f):
            for target_class in target_classes:
                GradSampleModule.GRAD_SAMPLERS[target_class] = f
                GradSampleModuleFastGradientClipping.GRAD_SAMPLERS[target_class] = f
            return f
        return decorator
    
    def register_norm_sampler(target_class_or_classes):
        def decorator(f):
            for target_class in target_classes:
                GradSampleModuleFastGradientClipping.NORM_SAMPLERS[target_class] = f
            return f
        return decorator
    '''
    print(decorator_code)

def demonstrate_performance_difference():
    """æ¼”ç¤ºæ€§èƒ½å·®å¼‚"""
    print(f"\n" + "=" * 80)
    print("æ€§èƒ½å·®å¼‚å¯¹æ¯”")
    print("=" * 80)
    
    performance_comparison = """
    ä¸‰ç§é‡‡æ ·ç­–ç•¥çš„æ€§èƒ½å¯¹æ¯”:
    
    ğŸš€ Ghost Clipping (ç¬¬217è¡Œè·¯å¾„):
    ------------------------------
    â€¢ å†…å­˜ä½¿ç”¨: æœ€ä½ (åªå­˜å‚¨èŒƒæ•°)
    â€¢ è®¡ç®—é€Ÿåº¦: æœ€å¿« (ä¸“é—¨ä¼˜åŒ–çš„èŒƒæ•°è®¡ç®—)
    â€¢ æ”¯æŒå±‚: æœ‰é™ (éœ€è¦ä¸“é—¨å®ç°)
    â€¢ é€‚ç”¨åœºæ™¯: ç”Ÿäº§ç¯å¢ƒï¼Œå¤§æ¨¡å‹è®­ç»ƒ
    
    âš¡ Fast Gradient Clipping (ç¬¬227è¡Œè·¯å¾„):
    -------------------------------------
    â€¢ å†…å­˜ä½¿ç”¨: ä¸­ç­‰ (ä¸´æ—¶å­˜å‚¨æ¢¯åº¦)
    â€¢ è®¡ç®—é€Ÿåº¦: ä¸­ç­‰ (éœ€è¦å®Œæ•´æ¢¯åº¦è®¡ç®—)
    â€¢ æ”¯æŒå±‚: è¾ƒå¤š (å¤§éƒ¨åˆ†å¸¸ç”¨å±‚)
    â€¢ é€‚ç”¨åœºæ™¯: å¹³è¡¡æ€§èƒ½å’Œå…¼å®¹æ€§
    
    ğŸŒ Functorch (fallbackè·¯å¾„):
    ---------------------------
    â€¢ å†…å­˜ä½¿ç”¨: ä¸­ç­‰ (ä¸´æ—¶å­˜å‚¨æ¢¯åº¦)
    â€¢ è®¡ç®—é€Ÿåº¦: è¾ƒæ…¢ (é€šç”¨å®ç°)
    â€¢ æ”¯æŒå±‚: æ‰€æœ‰å±‚ (é€šç”¨æ–¹æ³•)
    â€¢ é€‚ç”¨åœºæ™¯: å…¼å®¹æ€§ä¼˜å…ˆï¼Œæ–°å±‚ç±»å‹
    
    é€‰æ‹©ä¼˜å…ˆçº§: Ghost Clipping > Fast Gradient Clipping > Functorch
    """
    print(performance_comparison)

if __name__ == "__main__":
    print("GRAD_SAMPLERS å’Œ NORM_SAMPLERS æ³¨å†Œæœºåˆ¶è¯¦è§£")
    print("=" * 80)
    
    explain_samplers_registry()
    demonstrate_registry_content()
    explain_line_217_and_227()
    demonstrate_sampler_selection()
    show_actual_sampler_functions()
    explain_registration_process()
    demonstrate_performance_difference()
    
    print(f"\n" + "=" * 80)
    print("æ€»ç»“")
    print("=" * 80)
    print("å…³é”®è¦ç‚¹:")
    print("â€¢ ç¬¬217è¡Œ: é€‰æ‹©Ghost Clippingçš„èŒƒæ•°è®¡ç®—å‡½æ•° (æœ€é«˜æ•ˆ)")
    print("â€¢ ç¬¬227è¡Œ: é€‰æ‹©ä¼ ç»Ÿçš„æ¢¯åº¦è®¡ç®—å‡½æ•° (å…¼å®¹æ€§å¥½)")
    print("â€¢ æ³¨å†Œè¡¨é€šè¿‡è£…é¥°å™¨åœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨å¡«å……")
    print("â€¢ ä¼˜å…ˆçº§: NORM_SAMPLERS > GRAD_SAMPLERS > functorch")
    print("â€¢ è¿™ç§è®¾è®¡å®ç°äº†æ€§èƒ½å’Œå…¼å®¹æ€§çš„å®Œç¾å¹³è¡¡")
    print("=" * 80)