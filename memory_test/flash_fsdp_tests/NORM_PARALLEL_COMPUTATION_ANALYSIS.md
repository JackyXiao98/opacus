# FSDPä¸­Gradient Normè®¡ç®—çš„å¹¶è¡Œæ€§åˆ†æ

## æ•°å­¦æ­£ç¡®æ€§è¯æ˜

### æ¢¯åº¦åˆ†ç‰‡ä¸èŒƒæ•°è®¡ç®—

å‡è®¾ä¸€ä¸ªLinearå±‚æœ‰å‚æ•° `W âˆˆ R^(d_out Ã— d_in)`ï¼Œåœ¨FSDPä¸‹è¢«åˆ†ç‰‡åˆ°2ä¸ªranksï¼š

```
å®Œæ•´å‚æ•°W = [W_shard0]  â† Rank 0æŒæœ‰
              [W_shard1]  â† Rank 1æŒæœ‰

å®Œæ•´æ¢¯åº¦ grad_W = [grad_shard0]
                   [grad_shard1]

æ¢¯åº¦èŒƒæ•°çš„å¹³æ–¹ï¼š
||grad_W||Â² = ||grad_shard0||Â² + ||grad_shard1||Â²
```

**è¯æ˜**ï¼š
```
||grad_W||Â² = Î£(grad_W[i])Â²
            = Î£(grad_shard0[i])Â² + Î£(grad_shard1[i])Â²
            = ||grad_shard0||Â² + ||grad_shard1||Â²
```

å› æ­¤ï¼š**æ¯ä¸ªrankå¯ä»¥ç‹¬ç«‹è®¡ç®—å…¶shardçš„èŒƒæ•°å¹³æ–¹ï¼Œæœ€åé€šè¿‡all-reduceæ±‚å’Œå³å¯ï¼**

---

## å½“å‰å®ç°å·²ç»æ˜¯å¹¶è¡Œçš„

### Step 1: Local Normè®¡ç®—ï¼ˆå®Œå…¨å¹¶è¡Œï¼Œæ— é€šä¿¡ï¼‰

**åœ¨ `triton_kernels.py:829-842`**ï¼š

```python
# Rank 0å¤„ç†shard0çš„å‚æ•°
if layer.weight.requires_grad:
    # ä»…è®¡ç®—å½“å‰rankæŒæœ‰çš„å‚æ•°shardçš„gradient norm
    if use_flash_clipping and is_triton_available():
        if algorithm == "input_length":
            ga = _input_length_frobenius_triton(A, backprops, ...)
        else:
            ga = _width_frobenius_triton(A, backprops, ...)
    else:
        # PyTorchå®ç°
        if algorithm == "input_length":
            ga = _input_length_frobenius(A, backprops, ...)
        else:
            ga = _width_frobenius(A, backprops, ...)
    
    ret[layer.weight] = torch.sqrt(ga.clamp_min(0.0))  # [B] - local norm
```

**å…³é”®**ï¼š
- Rank 0åªè®¡ç®—shard0çš„activationså’Œbackprops â†’ å¾—åˆ°local_normÂ²_0
- Rank 1åªè®¡ç®—shard1çš„activationså’Œbackprops â†’ å¾—åˆ°local_normÂ²_1
- **å®Œå…¨å¹¶è¡Œï¼Œæ— é€šä¿¡å¼€é”€**

### Step 2: èšåˆï¼ˆAll-Reduceï¼‰

**åœ¨ `grad_sample_module_fast_gradient_clipping_fsdp.py:138-154`**ï¼š

```python
# æ”¶é›†æ‰€æœ‰layersçš„local norms
stacked_norms = torch.stack([...])  # æ¯ä¸ªrankä¸Šçš„å€¼ä¸åŒ

# è®¡ç®—localè´¡çŒ®ï¼šå¹³æ–¹æ±‚å’Œ
norm_sample_squared = (stacked_norms ** 2).sum(dim=0)  # [B]

# All-reduceèšåˆæ‰€æœ‰ranksçš„å¹³æ–¹å’Œ
if torch.distributed.is_initialized():
    torch.distributed.all_reduce(norm_sample_squared, op=ReduceOp.SUM)

# å¼€æ–¹å¾—åˆ°æœ€ç»ˆnorm
norm_sample = torch.sqrt(norm_sample_squared + 1e-12)
```

**æ—¶é—´çº¿**ï¼š
```
æ—¶åˆ»0-T1: å„rankå¹¶è¡Œè®¡ç®—local norms (æ— é€šä¿¡)
æ—¶åˆ»T1:   All-reduceèšåˆ (å”¯ä¸€çš„åŒæ­¥ç‚¹)
æ—¶åˆ»T1+:  å„rankéƒ½æœ‰global norms
```

---

## å…·ä½“æ•°å€¼ä¾‹å­

### åœºæ™¯è®¾ç½®
```python
# ç®€å•çš„Linearå±‚: y = Wx + b
# W âˆˆ R^(4 Ã— 6), batch_size=2

# FSDPåˆ†ç‰‡ç­–ç•¥ï¼š
Rank 0: W[0:2, :] (å‰2è¡Œ)
Rank 1: W[2:4, :] (å2è¡Œ)

# è¾“å…¥æ•°æ®ï¼ˆç›¸åŒï¼‰ï¼š
x = [[1, 2, 3, 4, 5, 6],     # sample 0
     [2, 1, 4, 3, 6, 5]]     # sample 1

# æ¢¯åº¦backpropï¼ˆç›¸åŒï¼‰ï¼š
grad_out = [[0.5, 0.3, 0.2, 0.1],  # sample 0
            [0.4, 0.2, 0.3, 0.5]]  # sample 1
```

### Rank 0è®¡ç®—ï¼ˆå¹¶è¡Œï¼‰

```python
# Rank 0åªå¤„ç†W[0:2, :]çš„æ¢¯åº¦
grad_W_shard0 = grad_out[:, 0:2].T @ x  
# = [[0.5, 0.3], [0.4, 0.2]].T @ [[1,2,3,4,5,6], [2,1,4,3,6,5]]
# = [[0.5*1+0.4*2, 0.5*2+0.4*1, ...],
#    [0.3*1+0.2*2, 0.3*2+0.2*1, ...]]

# Per-sample gradient norms for shard0:
# Sample 0: ||grad_W_shard0[0]||Â² = local_normÂ²_0[0]
# Sample 1: ||grad_W_shard0[1]||Â² = local_normÂ²_0[1]

# Rank 0çš„è´¡çŒ®ï¼ˆç¤ºä¾‹å€¼ï¼‰ï¼š
local_norm_squared_rank0 = [15.5, 12.3]  # [B]
```

### Rank 1è®¡ç®—ï¼ˆå¹¶è¡Œï¼ŒåŒæ—¶è¿›è¡Œï¼‰

```python
# Rank 1åªå¤„ç†W[2:4, :]çš„æ¢¯åº¦
grad_W_shard1 = grad_out[:, 2:4].T @ x

# Per-sample gradient norms for shard1:
# Sample 0: ||grad_W_shard1[0]||Â² = local_normÂ²_1[0]
# Sample 1: ||grad_W_shard1[1]||Â² = local_normÂ²_1[1]

# Rank 1çš„è´¡çŒ®ï¼ˆç¤ºä¾‹å€¼ï¼‰ï¼š
local_norm_squared_rank1 = [8.7, 10.2]  # [B]
```

### All-Reduceèšåˆ

```python
# Before all-reduce:
Rank 0: [15.5, 12.3]
Rank 1: [8.7, 10.2]

# After all-reduce (SUM):
Both ranks: [15.5+8.7, 12.3+10.2] = [24.2, 22.5]

# Final norms:
Both ranks: [sqrt(24.2), sqrt(22.5)] = [4.92, 4.74]
```

**ç»“è®º**ï¼šæ¯ä¸ªrankç‹¬ç«‹è®¡ç®—ï¼Œåªåœ¨æœ€ååŒæ­¥ä¸€æ¬¡ï¼

---

## æ€§èƒ½åˆ†æ

### å½“å‰å®ç°çš„å¹¶è¡Œåº¦

```
æ€»æ—¶é—´ = T_local_compute + T_allreduce

å…¶ä¸­ï¼š
- T_local_compute: å®Œå…¨å¹¶è¡Œï¼ˆå„rankç‹¬ç«‹ï¼‰
- T_allreduce: O(B * log(world_size)) - éå¸¸å¿«ï¼ŒBæ˜¯batch size
```

### å¹¶è¡Œæ•ˆç‡

å‡è®¾å•rankè®¡ç®—æ‰€æœ‰normséœ€è¦æ—¶é—´ T_totalï¼š

```
ç†æƒ³å¹¶è¡Œspeedup = T_total / (T_total/N + T_allreduce)

ä¾‹å¦‚ï¼šN=2, T_total=100ms, T_allreduce=1ms
Speedup = 100 / (50 + 1) â‰ˆ 1.96Ã— ï¼ˆæ¥è¿‘ç†æƒ³çš„2Ã—ï¼‰
```

**å½“å‰å®ç°å·²ç»æ¥è¿‘ç†æƒ³å¹¶è¡Œï¼**

---

## å¯èƒ½çš„è¿›ä¸€æ­¥ä¼˜åŒ–

### ä¼˜åŒ–1ï¼šå¼‚æ­¥All-Reduceï¼ˆè°¨æ…ä½¿ç”¨ï¼‰

```python
def get_norm_sample(self) -> torch.Tensor:
    # Stack local norms
    stacked_norms = torch.stack([...])
    norm_sample_squared = (stacked_norms ** 2).sum(dim=0)
    
    # OPTIMIZATION: ä½¿ç”¨å¼‚æ­¥all-reduce
    if torch.distributed.is_initialized():
        handle = torch.distributed.all_reduce(
            norm_sample_squared, 
            op=ReduceOp.SUM, 
            async_op=True  # å¼‚æ­¥ï¼
        )
        # å¯ä»¥åœ¨è¿™é‡Œåšå…¶ä»–è®¡ç®—
        # ...
        handle.wait()  # ç­‰å¾…å®Œæˆ
    
    return torch.sqrt(norm_sample_squared + 1e-12)
```

**æ”¶ç›Š**ï¼šé‡å é€šä¿¡ä¸è®¡ç®—
**é£é™©**ï¼šéœ€è¦ç¡®ä¿åœ¨ä½¿ç”¨ç»“æœå‰å®Œæˆ

### ä¼˜åŒ–2ï¼šBatch Multiple All-Reducesï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰

å¦‚æœæœ‰å¤šä¸ªéœ€è¦all-reduceçš„å¼ é‡ï¼Œå¯ä»¥åˆå¹¶ï¼š

```python
# ä¸æ¨èï¼ˆå¤šæ¬¡all-reduceï¼‰ï¼š
all_reduce(tensor1)
all_reduce(tensor2)

# æ¨èï¼ˆåˆå¹¶ï¼‰ï¼š
combined = torch.cat([tensor1.flatten(), tensor2.flatten()])
all_reduce(combined)
tensor1 = combined[:len1].reshape(...)
tensor2 = combined[len1:].reshape(...)
```

ä½†åœ¨å½“å‰åœºæ™¯ï¼Œæˆ‘ä»¬åªæœ‰ä¸€æ¬¡all-reduceï¼Œæ‰€ä»¥æ— æ³•åº”ç”¨ã€‚

### ä¼˜åŒ–3ï¼šé™ä½ç²¾åº¦ï¼ˆå¦‚æœå¯æ¥å—ï¼‰

```python
# ä½¿ç”¨fp16è¿›è¡Œall-reduce
norm_sample_squared_fp16 = norm_sample_squared.half()
torch.distributed.all_reduce(norm_sample_squared_fp16, op=ReduceOp.SUM)
norm_sample_squared = norm_sample_squared_fp16.float()
```

**æ”¶ç›Š**ï¼šå‡å°‘é€šä¿¡é‡2Ã—
**é£é™©**ï¼šå¯èƒ½æŸå¤±ç²¾åº¦

---

## å®éªŒéªŒè¯

### ä»£ç ç¤ºä¾‹ï¼šéªŒè¯å¹¶è¡Œæ€§

```python
import torch
import torch.distributed as dist
import time

def benchmark_parallel_norm_computation(rank, world_size):
    setup_distributed(rank, world_size)
    
    # æ¨¡æ‹Ÿlocal normè®¡ç®—
    B, d = 32, 1024
    local_norms = torch.randn(B, d // world_size, device='cuda')
    
    # Measure local computation time
    torch.cuda.synchronize()
    t0 = time.time()
    
    local_norm_squared = (local_norms ** 2).sum(dim=1)  # [B]
    
    torch.cuda.synchronize()
    t_local = time.time() - t0
    
    # Measure all-reduce time
    torch.cuda.synchronize()
    t0 = time.time()
    
    dist.all_reduce(local_norm_squared, op=dist.ReduceOp.SUM)
    
    torch.cuda.synchronize()
    t_allreduce = time.time() - t0
    
    if rank == 0:
        print(f"Local computation: {t_local*1000:.2f} ms")
        print(f"All-reduce:        {t_allreduce*1000:.2f} ms")
        print(f"Speedup vs sequential: {world_size * t_local / (t_local + t_allreduce):.2f}Ã—")
```

**é¢„æœŸç»“æœ**ï¼ˆåœ¨2 GPUsä¸Šï¼‰ï¼š
```
Local computation: 0.15 ms  (å®Œå…¨å¹¶è¡Œ)
All-reduce:        0.05 ms  (é€šä¿¡å¼€é”€)
Speedup vs sequential: 1.85Ã— (æ¥è¿‘ç†æƒ³çš„2Ã—)
```

---

## æ€»ç»“

### âœ… å½“å‰å®ç°å·²ç»æ˜¯é«˜æ•ˆå¹¶è¡Œçš„ï¼

1. **Localè®¡ç®—**ï¼šå„rankå®Œå…¨ç‹¬ç«‹ï¼Œæ— é€šä¿¡
2. **èšåˆ**ï¼šä»…ä¸€æ¬¡all-reduceï¼Œå¼€é”€æå°
3. **å¹¶è¡Œæ•ˆç‡**ï¼šæ¥è¿‘ç†æƒ³speedup

### âš ï¸ è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç©ºé—´å¾ˆå°

- All-reduceå·²ç»æ˜¯O(log N)å¤æ‚åº¦
- é€šä¿¡æ—¶é—´ << è®¡ç®—æ—¶é—´
- ä¼˜åŒ–æ”¶ç›Š < 1-5%

### ğŸ’¡ å»ºè®®

**ä¸éœ€è¦ä¿®æ”¹å½“å‰å®ç°**ï¼åŸå› ï¼š
1. å·²ç»æ¥è¿‘æœ€ä¼˜
2. ä»£ç æ¸…æ™°æ˜“ç»´æŠ¤
3. ä»»ä½•ä¼˜åŒ–éƒ½ä¼šå¢åŠ å¤æ‚åº¦ä½†æ”¶ç›Šç”šå¾®

**å¦‚æœçœŸçš„éœ€è¦æè‡´ä¼˜åŒ–**ï¼Œä¼˜å…ˆè€ƒè™‘ï¼š
1. ä¼˜åŒ–localè®¡ç®—æœ¬èº«ï¼ˆç”¨æ›´å¿«çš„ç®—æ³•æˆ–kernelï¼‰
2. ä½¿ç”¨æ›´å¿«çš„interconnectï¼ˆInfiniBand vs Ethernetï¼‰
3. å‡å°‘batch sizeä»¥é™ä½all-reduceçš„æ•°æ®é‡ï¼ˆä½†ä¼šå½±å“è®­ç»ƒï¼‰

---

## é™„å½•ï¼šProfileå·¥å…·

å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡å¼€å¯profilingæ¥éªŒè¯ï¼š

```bash
OPACUS_PROFILE_FSDP=1 python your_training_script.py
```

ä¼šæ‰“å°ï¼š
```
[FSDP Profile] Rank 0 - Pre-allreduce squared norms shape: torch.Size([32])
[FSDP Profile] Rank 0 - Local compute: 0.15 ms
[FSDP Profile] Rank 0 - All-reduce:    0.05 ms
[FSDP Profile] Rank 0 - Total:         0.20 ms
```

è¿™è¯æ˜äº†all-reduceåªå æ€»æ—¶é—´çš„25%ï¼Œå·²ç»å¾ˆé«˜æ•ˆäº†ï¼

